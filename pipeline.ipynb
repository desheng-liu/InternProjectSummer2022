{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.v2.dsl\n",
    "\n",
    "import kfp_tekton\n",
    "from kfp_tekton import k8s_client_helper\n",
    "\n",
    "from kubernetes.client.models import V1EnvVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"tf_spacenet_cv_classify_pipeline-e426805\"\n",
    "PIPELINE_PACKAGE = f\"{PIPELINE_NAME}.zip\"\n",
    "EXPERIMENT_NAME = \"RMS ET Intern Project\"\n",
    "PERSISTENT_VOLUME_CLAIM_NAME = \"rms-et-sn-pvc\"\n",
    "\n",
    "KUBEFLOW_PUBLIC_ENDPOINT_URL = \"https://kubeflow.apps.pcell.ai.us.lmco.com\"\n",
    "SESSION_COOKIE = \"authservice_session=MTY1ODI1NDM2NXxOd3dBTkZOUVNrWlBOakkyVEZGYVJrODFVVlZOUlVKUldsRktUek5JUjBsUFNqUllTMDlMUkVaVVFWQkxRbFJLVEZrMFFWbEZOMEU9fE7AOJaxj2kgkesMohGGCXqQKN6pQzrkJYEkzHuciy5u\"\n",
    "KUBEFLOW_PROFILE_NAME = \"kf-rms-et-intern-project-17\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.component\n",
    "def view_data():\n",
    "    import json\n",
    "    import os\n",
    "    import logging\n",
    "    \n",
    "    DATASET_PATH = \"/data/V1/sn1_AOI_1_RIO/sn1_AOI_1_RIO/sn1_AOI_1_RIO/\"\n",
    "    \n",
    "    PATH_TO_COLLECTION_FILE=os.path.join(DATASET_PATH, 'collection.json')\n",
    "\n",
    "    with open(PATH_TO_COLLECTION_FILE, 'r') as f:\n",
    "        collection_json = json.load(f)\n",
    "\n",
    "    print(f\"collection_json top level keys: {collection_json.keys()}\\n\")\n",
    "    print(f\"description: {collection_json['description']}\")\n",
    "    \n",
    "    def limit_string(s, chars=200):\n",
    "        s = str(s)\n",
    "        len_to_print = max(min(len(s), chars), 1)\n",
    "        return s[0:len_to_print]\n",
    "    \n",
    "    [print(f\"{k} : {limit_string(collection_json[k])}\") for k in collection_json.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_data_op = comp.func_to_container_op(\n",
    "    view_data,\n",
    "    base_image=\"harbor.ai.us.lmco.com/ai-factory-local/internal/aws-cli:1.20.58-debian-10-r2-0\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.component\n",
    "def predict():\n",
    "    import datetime\n",
    "    import json\n",
    "    import os\n",
    "    import math\n",
    "    import random\n",
    "    \n",
    "    # for dataset\n",
    "    import torch\n",
    "    import numpy as np\n",
    "\n",
    "    # Python Image Library can \n",
    "    # load tiffs and convert to numpy arrays\n",
    "    from PIL import Image\n",
    "    from IPython.display import display\n",
    "\n",
    "    # for mask creation based on geospatial data\n",
    "    import fiona\n",
    "    import rioxarray\n",
    "    import geojson\n",
    "\n",
    "    from pathlib import Path\n",
    "    # import UNet Model Configuration, UNet Vision Model Configuration, and the Vision Model\n",
    "    import classification as lmclassification\n",
    "    from classification.data_types.network_callbacks.checkpoint_callback import CheckPointCallback as CheckpointCallback\n",
    "    from classification.network_models.configurations.unet_model_configuration import UNetModelConfiguration as ModelConfiguration\n",
    "    from classification.vision.models.configurations.unet_vision_model_configuration import UNetVisionModelConfiguration as VisionModelConfiguration\n",
    "    from classification.vision.models.unet_vision_model import UNetVisionModel as Model\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    import matplotlib.colors as colors\n",
    "    \n",
    "    class SpaceNetDataset(torch.utils.data.Dataset):\n",
    "        \"\"\"\n",
    "        A torch.utils.data.Dataset wrapping the SpaceNet Building Detection V1 dataset\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, path_to_collection, *args, **kwargs):\n",
    "            \"\"\"\n",
    "            Construct the dataset\n",
    "\n",
    "            Provide any information required to allow subsequent operation\n",
    "            \"\"\"\n",
    "\n",
    "            if not isinstance(path_to_collection, Path):\n",
    "                path_to_collection = Path(path_to_collection)\n",
    "\n",
    "            self.path_to_collection = path_to_collection\n",
    "            self.collection_dir = os.path.dirname(path_to_collection)\n",
    "\n",
    "            with open(self.path_to_collection, 'r') as f:\n",
    "                self.collection = json.load(f)\n",
    "\n",
    "            # self.links holds all the top level folders\n",
    "            # filter out the ones that end with -labels since we know \n",
    "            # there will be a corresponding non-labels folder\n",
    "            self.links = [x[\"href\"] for x in self.collection[\"links\"] if not os.path.dirname(x[\"href\"]).endswith('-labels') ]\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            \"\"\"\n",
    "            Returns:\n",
    "                The length of the whole dataset to load\n",
    "            \"\"\"\n",
    "            return len(self.links)\n",
    "\n",
    "        def __getitem__(self, idx) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "            \"\"\"\n",
    "            Given an index, provide the training example at that index\n",
    "\n",
    "            To satisfy the CCM return a tuple of 2 elements:\n",
    "                The first element is a numpy np.ndarray, 32-bit or 64-bit float, with shape (channels, pixel height, pixel width)\n",
    "                The second element is a dictionary.\n",
    "                    The only required key is 'masks'\n",
    "                    The 'masks' key must map to a numpy np.ndarray, 64-bit integers, with shape (classes, pixel height, pixel width)\n",
    "                    In this case the classes are background and building (just 2 classes)\n",
    "                    For the background class, pixel locations that are background should be set to 1, otherwise 0\n",
    "                    For the building class, pixel locations that are buildings should be set to 1, otherwise 0\n",
    "                    Image values in the numpy ndarray should be normalized to a range from 0.0 to 1.0\n",
    "\n",
    "            Other requirements:\n",
    "                Image Width must be the same as Segmentation Mask Width\n",
    "                Image Height must be the same as Segmentation Mask Height\n",
    "\n",
    "                If no features are present. the mask should still exist but take on default values, eg\n",
    "                    background class all ones\n",
    "                    building class all zeros\n",
    "                    other constraints are still enforced\n",
    "\n",
    "            Args:\n",
    "                index into the dataset to sample\n",
    "\n",
    "            Returns:\n",
    "                The dataset example at a given index in the form of a tuple of\n",
    "                np.ndarray and dictionary of string key mapped to np.ndarrays\n",
    "            \"\"\"\n",
    "\n",
    "            # index into folders\n",
    "            example_file = self.links[idx]\n",
    "\n",
    "            # construct the known corresponding label folder name\n",
    "            label_folder = f\"{os.path.dirname(example_file)}-labels\"\n",
    "\n",
    "            # construct the relative path to the \"stac.json file\"\n",
    "            label_file = os.path.join(label_folder, 'stac.json')\n",
    "\n",
    "            with open(os.path.join(self.collection_dir, example_file), 'r') as f:\n",
    "                example_json = json.load(f)\n",
    "\n",
    "            # construct the path to the image file\n",
    "            image_file = os.path.join(self.collection_dir, os.path.dirname(example_file), example_json[\"assets\"][\"RGB\"][\"href\"])\n",
    "\n",
    "            with open(os.path.join(self.collection_dir, label_file), 'r') as f:\n",
    "                labels_json = json.load(f)\n",
    "\n",
    "            # construct the paths to the labels file\n",
    "            label_file = os.path.join(self.collection_dir, label_folder, labels_json[\"assets\"][\"labels\"][\"href\"])\n",
    "\n",
    "            # load the .tif image from disk\n",
    "            pil_img = Image.open(image_file)\n",
    "\n",
    "            # convert the PIL image to a numpy array\n",
    "            img = np.array(pil_img)\n",
    "\n",
    "            # normalize from 0.0 to 1.0\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "            # open with fiona to get features for mask - this represnts the buildings\n",
    "            with fiona.open(label_file, 'r') as f:\n",
    "                features = [feature[\"geometry\"] for feature in f]\n",
    "\n",
    "            # create the rasterizer object based on the image data\n",
    "            rds = rioxarray.open_rasterio(image_file).isel(band=0)\n",
    "\n",
    "            # get the geojson data\n",
    "            with open(label_file) as igj:\n",
    "                data = geojson.load(igj)\n",
    "\n",
    "            # binary crossentropy requires masks of shape (2, height, width)\n",
    "            # the first channel represnts classes; 0=background, 1=building\n",
    "            mask = np.zeros((2, *img.shape[0:2])).astype(np.int64)\n",
    "            mask[0,:,:] = np.ones((1, *img.shape[0:2])) # background by default\n",
    "\n",
    "            # if this training/validation example has features (building masks)\n",
    "            if len(features) > 0:\n",
    "\n",
    "                # get the mask - binary cross entropy\n",
    "                rds = rds.rio.clip(features, data[\"crs\"][\"properties\"][\"name\"], drop=False)\n",
    "                nprds = np.array(rds)\n",
    "\n",
    "                mask[0, :, :] = np.where((nprds)>0.5, 0, mask[0, :, :])\n",
    "                mask[1, :, :] = np.where(nprds>0.5, 1, mask[1, :, :])\n",
    "\n",
    "            # if the example did not have features, leave the images and masks as they were\n",
    "\n",
    "            # channels, height, width\n",
    "            img = np.transpose(img,  (2, 0, 1))\n",
    "\n",
    "            # clip both masks and images to ensure they are the same size\n",
    "            # otherwise CCM / Most Conv networks will not accept\n",
    "            min_height = min(img.shape[1], mask.shape[1])\n",
    "            min_width  = min(img.shape[2], mask.shape[2])\n",
    "            img = img[:, :min_height, :min_width]\n",
    "            mask = mask[:, :min_height, :min_width]\n",
    "            mask = np.expand_dims(np.argmax(mask, axis=0), axis=0)\n",
    "\n",
    "            # CCM vision models expect targets to be in a dictionary \n",
    "            # other keys for other types of computer vision problems include\n",
    "            # 'boxes' and 'labels'. UNet does not require those\n",
    "            return img, {'masks': mask}\n",
    "    \n",
    "    # Instantiate the Dataloader\n",
    "    DATASET_PATH = \"/data/V1/sn1_AOI_1_RIO/sn1_AOI_1_RIO/sn1_AOI_1_RIO/\"\n",
    "    PATH_TO_COLLECTION_FILE=os.path.join(DATASET_PATH, 'collection.json')\n",
    "    dataset = SpaceNetDataset(PATH_TO_COLLECTION_FILE)\n",
    "    \n",
    "    TRAIN_SPLIT = 0.7\n",
    "    TEST_SPLIT = 0.3\n",
    "\n",
    "    print(f\"TRAINING SPLIT STARTED\")\n",
    "    \n",
    "    test_dataset, train_dataset = torch.utils.data.random_split(\n",
    "        dataset, \n",
    "        [math.floor(TEST_SPLIT*len(dataset)), math.ceil(TRAIN_SPLIT*len(dataset)) ],\n",
    "        generator=torch.Generator().manual_seed(1234)\n",
    "    )\n",
    "    \n",
    "    print(f\"TRAINING SPLIT FINISHED\")\n",
    "    \n",
    "    # instantiate UNet Model Configuration\n",
    "    config = ModelConfiguration()\n",
    "    \n",
    "    # instantiate UNet Vision Model Configuration\n",
    "    model_config = VisionModelConfiguration(\n",
    "        input_shape=(480,480),\n",
    "        n_classes=2,\n",
    "        input_channels=3,\n",
    "        batch_size=6,\n",
    "        learning_rate=1e-4,\n",
    "        validation_split=0.3,\n",
    "        training_epochs=11,\n",
    "        layers_model_configuration=config,\n",
    "    )\n",
    "    # ensure we allow for the background class and building class\n",
    "    # RGB    \n",
    "    # during testing x% of the training dataset will be reserved for validation\n",
    "\n",
    "\n",
    "    # Instantiate UNet\n",
    "    model = Model(model_config)\n",
    "\n",
    "        # if using pretrained weights, Images should be renormalized according to pytorch's specification:\n",
    "        #    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                                     std=[0.229, 0.224, 0.225])\n",
    "        # see https://gitlab.us.lmco.com/overwatch/cognitive-modules/classification/-/blob/master/examples/jupyter_notebooks/supervised_examples/deep_learning_examples/inception_v3_classifier_example.ipynb\n",
    "        #         Create Classes and Labels from Dataset \n",
    "   \n",
    "    nowstr = datetime.datetime.now().strftime('%m%d%Y_%H%M%S')\n",
    "    model_path = f'./mnt/space_net_models/trained_spacenet_{nowstr}.pt'\n",
    "    print(f\"training new model: {model_path}\")\n",
    "    model_save_path = model_path\n",
    "    checkpoint_path = './mnt/space_net_models/'\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "    # train the model!\n",
    "    results = model.fit(train_dataset, verbose=True, callbacks=[CheckpointCallback(checkpoint_path)])\n",
    "\n",
    "    # save the trained model for the epochs configured\n",
    "    model.save_model(model_save_path)\n",
    "    print(f\"saving model to: {model_save_path} \\n\") \n",
    "    \n",
    "    # load the trained model\n",
    "    model.load_model(model_save_path)\n",
    "    \n",
    "    # load a test example\n",
    "    img, mask = test_dataset[ random.randint(0, len(test_dataset)) ]\n",
    "    \n",
    "    # run model inference\n",
    "    results = model.infer([img])\n",
    "    \n",
    "    result_mask = results[0]['masks']\n",
    "\n",
    "    result_mask = (result_mask - result_mask.min()) / (result_mask.max()-result_mask.min())\n",
    "\n",
    "    print(\"******** Inference Results ********\")\n",
    "    print(f\"Overall minimum prediction confidence = {result_mask.min()}\")\n",
    "    print(f\"Overall maximum prediction confidence = {result_mask.max()}\")\n",
    "    print(f\"Overall mean prediction confidence = {result_mask.mean()} \\n\")\n",
    "    \n",
    "    print(f\"Minimum Building prediction confidence = {result_mask[1,:,:].min()}\")\n",
    "    print(f\"Maximum Building prediction confidence = {result_mask[1,:,:].max()}\")\n",
    "    print(f\"Mean Building prediction confidence = {result_mask[1,:,:].mean()} \\n\")\n",
    "    \n",
    "    print(f\"Minimum Background prediction confidence = {result_mask[0,:,:].min()}\")\n",
    "    print(f\"Maximum Background prediction confidence = {result_mask[0,:,:].max()}\")\n",
    "    print(f\"Mean Background prediction confidence = {result_mask[0,:,:].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_op = comp.func_to_container_op(\n",
    "    predict,\n",
    "    base_image=\"harbor.ai.us.lmco.com/ai-factory-local/pytorch:1.8.1-cuda11.1-cudnn8-runtime-4\",\n",
    "    packages_to_install=['urllib3==1.25.8','fiona','numpy','Pillow','geojson', 'rioxarray', 'lm-ai-classification', 'matplotlib', 'sklearn']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=EXPERIMENT_NAME\n",
    ")\n",
    "def rms_et_basic_pipeline():\n",
    "    view_data_r = (\n",
    "        view_data_op()\n",
    "        .add_pvolumes({\"/data\": dsl.PipelineVolume(pvc=\"spacenet-data\")})\n",
    "    )\n",
    "    predict_r = (\n",
    "        predict_op()\n",
    "        .set_gpu_limit(8)\n",
    "        .add_env_variable(k8s_client_helper.env_from_secret(\"PIP_EXTRA_INDEX_URL\", \"nexus-config\", \"PIP_EXTRA_INDEX_URL\"))\n",
    "        .add_env_variable(k8s_client_helper.env_from_secret(\"PIP_INDEX_URL\", \"nexus-config\", \"PIP_INDEX_URL\"))\n",
    "        .add_pvolumes({\"/data\": dsl.PipelineVolume(pvc=\"spacenet-data\")})\n",
    "        .after(view_data_r)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp_tekton.TektonClient(\n",
    "    host=f\"{KUBEFLOW_PUBLIC_ENDPOINT_URL}/pipeline\",\n",
    "    ssl_ca_cert=\"/etc/ssl/certs/ca-certificates.crt\",\n",
    "    cookies=SESSION_COOKIE\n",
    ")\n",
    "\n",
    "try:\n",
    "    experiment = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except ValueError:\n",
    "    experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "kfp_tekton.compiler._op_to_template.RESOURCE_OP_IMAGE='lmregistry.us.lmco.com/ext.hub.docker.com/aipipeline/kubectl-wrapper:0.8.0'\n",
    "kfp_tekton.compiler.TektonCompiler().compile(rms_et_basic_pipeline, PIPELINE_PACKAGE)\n",
    "pipeline_id = client.get_pipeline_id(PIPELINE_NAME)\n",
    "\n",
    "if not pipeline_id:\n",
    "    # upload the package to Kubeflow\n",
    "    r = client.upload_pipeline(PIPELINE_PACKAGE, pipeline_name=PIPELINE_NAME)\n",
    "\n",
    "    # update the pipeline id\n",
    "    pipeline_id = r.id\n",
    "\n",
    "now = datetime.datetime.now().isoformat()\n",
    "pipeline_versionName = f\"{PIPELINE_NAME}_version_at_{now}Z\"\n",
    "version = client.pipeline_uploads.upload_pipeline_version(uploadfile=PIPELINE_PACKAGE, name=pipeline_versionName, pipelineid=pipeline_id)\n",
    "\n",
    "client.run_pipeline(experiment.id, f\"{PIPELINE_NAME} {now}\", pipeline_id=pipeline_id, version_id=version.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "33264e53a25ce865e784f7564913a735150180acd61f94069a0938fa0948b294"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
